=== Estructura de directorios y archivos ===
├── proxy_pb2.py
├── main.py
├── Dockerfile
├── README.md
├── scripts
│   ├── generate_proto.sh
├── proxy_pb2_grpc.py
├── api
│   ├── server.py
├── docker-compose.yml
├── script.sh
├── internal
│   ├── __init__.py
│   ├── proxy
│   │   ├── __init__.py
│   │   ├── proxy.py
│   ├── scraper
│   │   ├── scraper.py
│   ├── config
│   │   ├── sessions.py
│   │   ├── __init__.py
│   │   ├── config.py
├── protos
│   ├── proxy.proto

=== Contenido de archivos ===
----- [Archivo: api/server.py] -----
"""
Servidor gRPC para el proxy service
"""
import grpc
from concurrent import futures
import logging
import random
import requests
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

import proxy_pb2
import proxy_pb2_grpc
from internal.config.sessions import PROXY_SESSIONS
from internal.proxy.proxy import ProxyValidator
from internal.scraper.scraper import scrape_user_agents

logger = logging.getLogger(__name__)

class ProxyServicer(proxy_pb2_grpc.ProxyServiceServicer):
    def __init__(self):
        self.proxy_validator = ProxyValidator()
        self.valid_proxies = {}
        self.successful_proxies = {}
        self.user_agents = []
        self._initialize()
    
    def _initialize(self):
        """Inicializar el servidor con proxies y user agents"""
        logger.info("Inicializando servidor...")
        self.valid_proxies = self.proxy_validator.get_valid_proxies()
        self.user_agents = scrape_user_agents()
        logger.info(f"Servidor inicializado con {len(self.user_agents)} user agents")
    
    def _get_chrome_options(self, proxy_addr=None):
        """Configurar opciones de Chrome con proxy opcional"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        if proxy_addr:
            options.add_argument(f'--proxy-server=http://{proxy_addr}')
        
        return options
    
    def _create_driver(self, proxy_addr=None, timeout=10):
        """Crear driver de Selenium con configuración específica"""
        options = self._get_chrome_options(proxy_addr)
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.set_page_load_timeout(timeout)
            return driver
        except Exception as e:
            logger.error(f"Error creando driver: {e}")
            return None
    
    def _fetch_with_selenium(self, url, session_name, proxy_addr=None, user_agent=None):
        """Obtener contenido usando Selenium"""
        session_config = PROXY_SESSIONS.get(session_name)
        if not session_config:
            raise ValueError(f"Sesión '{session_name}' no encontrada")
        
        timeout = session_config.timeout / 1000  # Convertir a segundos
        driver = None
        
        try:
            driver = self._create_driver(proxy_addr, timeout)
            if not driver:
                raise Exception("No se pudo crear el driver")
            
            # Configurar user agent si se proporciona
            if user_agent:
                driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                    "userAgent": user_agent
                })
            
            # Configurar headers adicionales
            if session_config.headers:
                for header, value in session_config.headers.items():
                    driver.execute_cdp_cmd('Network.setRequestInterception', {
                        'patterns': [{'urlPattern': '*'}]
                    })
            
            # Navegar a la URL
            driver.get(url)
            
            # Esperar a que la página cargue
            WebDriverWait(driver, timeout).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Obtener el contenido
            content = driver.page_source.encode('utf-8')
            
            if proxy_addr:
                logger.info(f"Selenium - Proxy: {proxy_addr}, URL: {url}, Content length: {len(content)}")
            else:
                logger.info(f"Selenium - Direct, URL: {url}, Content length: {len(content)}")
            
            return content
            
        except TimeoutException:
            raise Exception("Timeout esperando a que cargue la página")
        except WebDriverException as e:
            raise Exception(f"Error de WebDriver: {str(e)}")
        except Exception as e:
            raise Exception(f"Error en Selenium: {str(e)}")
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def _fetch_with_requests(self, url, session_name, proxy_addr=None, user_agent=None):
        """Obtener contenido usando requests como fallback"""
        session_config = PROXY_SESSIONS.get(session_name)
        if not session_config:
            raise ValueError(f"Sesión '{session_name}' no encontrada")
        
        headers = session_config.headers.copy()
        if user_agent:
            headers['User-Agent'] = user_agent
        
        proxies = None
        if proxy_addr:
            proxies = {
                'http': f'http://{proxy_addr}',
                'https': f'http://{proxy_addr}'
            }
        
        timeout = session_config.timeout / 1000
        
        response = requests.get(
            url,
            headers=headers,
            proxies=proxies,
            timeout=timeout,
            allow_redirects=True
        )
        
        response.raise_for_status()
        return response.content
    
    def FetchContent(self, request, context):
        """Implementación del método FetchContent"""
        try:
            if not request.session or request.session not in PROXY_SESSIONS:
                context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
                context.set_details("Sesión inválida")
                return proxy_pb2.Response()
            
            # Seleccionar user agent aleatorio
            user_agent = None
            if self.user_agents:
                user_agent = random.choice(self.user_agents)
            
            if request.proxy and request.session in self.valid_proxies:
                # Intentar con proxies válidos
                proxies = self.valid_proxies[request.session]
                if proxies:
                    proxy_addr = random.choice(proxies)
                    try:
                        content = self._fetch_with_selenium(
                            request.url, 
                            request.session, 
                            proxy_addr, 
                            user_agent
                        )
                        return proxy_pb2.Response(content=content)
                    except Exception as e:
                        logger.warning(f"Error con proxy {proxy_addr}: {e}")
                        # Fallback a requests
                        try:
                            content = self._fetch_with_requests(
                                request.url, 
                                request.session, 
                                proxy_addr, 
                                user_agent
                            )
                            return proxy_pb2.Response(content=content)
                        except:
                            pass
            
            # Fallback: sin proxy
            try:
                content = self._fetch_with_selenium(
                    request.url, 
                    request.session, 
                    None, 
                    user_agent
                )
                return proxy_pb2.Response(content=content)
            except Exception as e:
                logger.warning(f"Error con Selenium directo: {e}")
                # Fallback final a requests
                content = self._fetch_with_requests(
                    request.url, 
                    request.session, 
                    None, 
                    user_agent
                )
                return proxy_pb2.Response(content=content)
                
        except Exception as e:
            logger.error(f"Error en FetchContent: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return proxy_pb2.Response()
    
    def GetRandomProxy(self, request, context):
        """Obtener un proxy aleatorio de una sesión específica"""
        try:
            if not request.session:
                context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
                context.set_details("La sesión no puede estar vacía")
                return proxy_pb2.ProxyResponse()
            
            if request.session not in PROXY_SESSIONS:
                return proxy_pb2.ProxyResponse(
                    proxy="",
                    success=False,
                    message=f"Sesión '{request.session}' no encontrada en configuración"
                )
            
            proxies = self.valid_proxies.get(request.session, [])
            if not proxies:
                return proxy_pb2.ProxyResponse(
                    proxy="",
                    success=False,
                    message=f"No hay proxies válidos disponibles para la sesión '{request.session}'"
                )
            
            selected_proxy = random.choice(proxies)
            logger.info(f"Proxy aleatorio seleccionado para sesión '{request.session}': {selected_proxy}")
            
            return proxy_pb2.ProxyResponse(
                proxy=selected_proxy,
                success=True,
                message=f"Proxy seleccionado exitosamente para la sesión '{request.session}'"
            )
            
        except Exception as e:
            logger.error(f"Error en GetRandomProxy: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return proxy_pb2.ProxyResponse()
    
    def GetProxyStats(self, request, context):
        """Obtener estadísticas de proxies por sesión"""
        try:
            stats = {}
            total_proxies = 0
            
            for session, proxies in self.valid_proxies.items():
                count = len(proxies)
                stats[session] = count
                total_proxies += count
            
            return proxy_pb2.StatsResponse(
                proxy_count_by_session=stats,
                total_valid_proxies=total_proxies
            )
            
        except Exception as e:
            logger.error(f"Error en GetProxyStats: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return proxy_pb2.StatsResponse()

def start_grpc_server():
    """Iniciar el servidor gRPC"""
    logger.info("Iniciando servidor gRPC en puerto 5000")
    
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    proxy_pb2_grpc.add_ProxyServiceServicer_to_server(ProxyServicer(), server)
    
    listen_addr = '[::]:5000'
    server.add_insecure_port(listen_addr)
    
    server.start()
    logger.info(f"Servidor gRPC iniciado en {listen_addr}")
    
    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Cerrando servidor gRPC...")
        server.stop(0)
----- [Archivo: docker-compose.yml] -----
services:
  proxy-server:
    build: .
    container_name: proxy_server_python
    ports:
      - "5000:5000"
    restart: always
    environment:
      - DISPLAY=:99
    volumes:
      - /dev/shm:/dev/shm
    networks:
      - proxy_network

networks:
  proxy_network:
    driver: bridge
    name: proxy_network
----- [Archivo: Dockerfile] -----
## Dockerfile
FROM python:3.11-slim

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    unzip \
    curl \
    xvfb \
    && rm -rf /var/lib/apt/lists/*

# Instalar Chrome
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list \
    && apt-get update \
    && apt-get install -y google-chrome-stable \
    && rm -rf /var/lib/apt/lists/*

# Establecer directorio de trabajo
WORKDIR /app

# Copiar requirements y instalar dependencias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar el código fuente
COPY . .

# Generar archivos proto
RUN python -m grpc_tools.protoc \
    --proto_path=protos \
    --python_out=. \
    --grpc_python_out=. \
    protos/proxy.proto

# Exponer puerto
EXPOSE 5000

# Comando de inicio
CMD ["python", "main.py"]
----- [Archivo: internal/config/config.py] -----
"""
Configuración general del sistema
"""

# Tamaño del chunk de proxies para procesamiento
DEFAULT_CHUNK_SIZE = 20

# Timeout por defecto para sesiones (en milisegundos)
DEFAULT_SESSION_TIMEOUT = 2000

# Tiempo de actualización de proxies (en minutos)
UPDATE_TIME_MINUTES = 30

# Configuración de Selenium
SELENIUM_TIMEOUT = 10  # segundos
SELENIUM_RETRIES = 3

# User agents por defecto
DEFAULT_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]
----- [Archivo: internal/config/__init__.py] -----

----- [Archivo: internal/config/sessions.py] -----
"""
Configuración de sesiones para diferentes sitios web
"""
from dataclasses import dataclass
from typing import Dict
from .config import DEFAULT_SESSION_TIMEOUT

@dataclass
class ProxySession:
    name: str
    url: str
    headers: Dict[str, str]
    timeout: int = DEFAULT_SESSION_TIMEOUT

# Sesiones configuradas
PROXY_SESSIONS = {
    "CoinMarketCap": ProxySession(
        name="CoinMarketCap",
        url="https://coinmarketcap.com/es/",
        headers={
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "es-ES,es;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        },
        timeout=DEFAULT_SESSION_TIMEOUT,
    ),
}

def get_headers_from_session(session_name: str) -> Dict[str, str]:
    """Obtener headers de una sesión específica"""
    session = PROXY_SESSIONS.get(session_name)
    return session.headers if session else {}
----- [Archivo: internal/__init__.py] -----

----- [Archivo: internal/proxy/__init__.py] -----

----- [Archivo: internal/proxy/proxy.py] -----
"""
Validación y gestión de proxies usando Selenium
"""
import logging
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

from ..config.config import DEFAULT_CHUNK_SIZE, SELENIUM_TIMEOUT
from ..config.sessions import PROXY_SESSIONS
from ..scraper.scraper import scrape_proxies

logger = logging.getLogger(__name__)

class ProxyValidator:
    def __init__(self, chunk_size: int = DEFAULT_CHUNK_SIZE):
        self.chunk_size = chunk_size
        self.valid_proxies: Dict[str, List[str]] = {}
        self._lock = threading.RLock()
    
    def _get_chrome_options(self, proxy_addr: str) -> Options:
        """Configurar opciones de Chrome para validación de proxies"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-logging')
        options.add_argument('--disable-extensions')
        options.add_argument('--window-size=1920,1080')
        options.add_argument(f'--proxy-server=http://{proxy_addr}')
        
        # Configuraciones adicionales para mayor estabilidad
        options.add_argument('--no-first-run')
        options.add_argument('--disable-default-apps')
        options.add_argument('--disable-popup-blocking')
        
        return options
    
    def _create_driver(self, proxy_addr: str) -> webdriver.Chrome:
        """Crear driver de Chrome configurado con proxy"""
        options = self._get_chrome_options(proxy_addr)
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.set_page_load_timeout(SELENIUM_TIMEOUT)
            return driver
        except Exception as e:
            logger.debug(f"Error creando driver para proxy {proxy_addr}: {e}")
            raise
    
    def _test_proxy_with_session(self, proxy_addr: str, session_name: str, session_config) -> bool:
        """Probar un proxy específico con una sesión específica usando Selenium"""
        driver = None
        try:
            driver = self._create_driver(proxy_addr)
            
            # Navegar a la URL de prueba
            driver.get(session_config.url)
            
            # Esperar a que la página cargue
            WebDriverWait(driver, SELENIUM_TIMEOUT).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Verificar que el contenido es válido
            page_source = driver.page_source
            if len(page_source) > 100 and "error" not in page_source.lower():
                logger.debug(f"Proxy {proxy_addr} válido para {session_name}")
                return True
            else:
                logger.debug(f"Proxy {proxy_addr} contenido inválido para {session_name}")
                return False
                
        except TimeoutException:
            logger.debug(f"Timeout con proxy {proxy_addr} para {session_name}")
            return False
        except WebDriverException as e:
            logger.debug(f"WebDriver error con proxy {proxy_addr} para {session_name}: {e}")
            return False
        except Exception as e:
            logger.debug(f"Error con proxy {proxy_addr} para {session_name}: {e}")
            return False
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def _test_proxy_all_sessions(self, proxy_addr: str) -> None:
        """Probar un proxy con todas las sesiones configuradas"""
        for session_name, session_config in PROXY_SESSIONS.items():
            if self._test_proxy_with_session(proxy_addr, session_name, session_config):
                with self._lock:
                    if session_name not in self.valid_proxies:
                        self.valid_proxies[session_name] = []
                    self.valid_proxies[session_name].append(proxy_addr)
    
    def _chunk_proxies(self, proxies: List[str]) -> List[List[str]]:
        """Dividir lista de proxies en chunks más manejables"""
        chunks = []
        for i in range(0, len(proxies), self.chunk_size):
            end = i + self.chunk_size
            if end > len(proxies):
                end = len(proxies)
            chunks.append(proxies[i:end])
        return chunks
    
    def get_valid_proxies(self) -> Dict[str, List[str]]:
        """Obtener y validar proxies usando Selenium"""
        logger.info("Iniciando validación de proxies con Selenium...")
        
        # Limpiar proxies válidos anteriores
        with self._lock:
            self.valid_proxies.clear()
        
        # Obtener lista de proxies
        proxies = scrape_proxies()
        if not proxies:
            logger.warning("No se obtuvieron proxies para validar")
            return {}
        
        logger.info(f"Validando {len(proxies)} proxies...")
        
        # Dividir en chunks para procesamiento
        chunks = self._chunk_proxies(proxies)
        
        # Procesar chunks con ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=5) as executor:
            chunk_futures = []
            
            for chunk_idx, chunk in enumerate(chunks):
                future = executor.submit(self._process_chunk, chunk, chunk_idx, len(chunks))
                chunk_futures.append(future)
            
            # Esperar a que terminen todos los chunks
            for future in as_completed(chunk_futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Error procesando chunk: {e}")
        
        # Mostrar estadísticas finales
        with self._lock:
            total_valid = sum(len(proxies) for proxies in self.valid_proxies.values())
            logger.info(f"Validación completada. Total de proxies válidos: {total_valid}")
            
            for session, proxy_list in self.valid_proxies.items():
                logger.info(f"Sesión {session}: {len(proxy_list)} proxies válidos")
            
            return self.valid_proxies.copy()
    
    def _process_chunk(self, chunk: List[str], chunk_idx: int, total_chunks: int) -> None:
        """Procesar un chunk de proxies"""
        logger.info(f"Procesando chunk {chunk_idx + 1}/{total_chunks} ({len(chunk)} proxies)")
        
        # Procesar proxies del chunk con ThreadPoolExecutor más pequeño
        with ThreadPoolExecutor(max_workers=3) as executor:
            proxy_futures = []
            
            for proxy in chunk:
                future = executor.submit(self._test_proxy_all_sessions, proxy)
                proxy_futures.append(future)
            
            # Esperar a que terminen todos los proxies del chunk
            for future in as_completed(proxy_futures):
                try:
                    future.result()
                except Exception as e:
                    logger.debug(f"Error validando proxy: {e}")
        
        logger.info(f"Chunk {chunk_idx + 1}/{total_chunks} completado")
----- [Archivo: internal/scraper/scraper.py] -----
"""
Scraping de proxies y user agents
"""
import logging
import requests
from typing import List
import time

logger = logging.getLogger(__name__)

def scrape_proxies() -> List[str]:
    """Scraping de proxies desde fuentes públicas"""
    proxy_urls = [
        "https://raw.githubusercontent.com/officialputuid/KangProxy/refs/heads/KangProxy/https/https.txt",
        "https://raw.githubusercontent.com/vakhov/fresh-proxy-list/refs/heads/master/https.txt",
        # Comentadas las fuentes menos confiables
        # "https://raw.githubusercontent.com/proxifly/free-proxy-list/main/proxies/protocols/http/data.txt",
        # "https://raw.githubusercontent.com/proxifly/free-proxy-list/refs/heads/main/proxies/all/data.txt",
    ]
    
    all_proxies = []
    
    for url in proxy_urls:
        try:
            logger.info(f"Obteniendo proxies de {url}")
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            lines = response.text.strip().split('\n')
            valid_proxies = []
            
            for line in lines:
                line = line.strip()
                if line and ':' in line:
                    # Limpiar formato de proxy si tiene múltiples ':'
                    parts = line.split(':')
                    if len(parts) >= 2:
                        proxy = f"{parts[0]}:{parts[1]}"
                        valid_proxies.append(proxy)
            
            all_proxies.extend(valid_proxies)
            logger.info(f"Obtenidos {len(valid_proxies)} proxies de {url}")
            
        except requests.RequestException as e:
            logger.warning(f"Error obteniendo proxies de {url}: {e}")
        except Exception as e:
            logger.error(f"Error inesperado con {url}: {e}")
    
    # Eliminar duplicados manteniendo orden
    unique_proxies = list(dict.fromkeys(all_proxies))
    logger.info(f"Total de proxies únicos obtenidos: {len(unique_proxies)}")
    
    return unique_proxies

def scrape_user_agents() -> List[str]:
    """Scraping de user agents"""
    user_agent_urls = [
        "https://gist.githubusercontent.com/pzb/b4b6f57144aea7827ae4/raw/cf847b76a142955b1410c8bcef3aabe221a63db1/user-agents.txt",
    ]
    
    all_user_agents = []
    max_retries = 3
    
    for url in user_agent_urls:
        for attempt in range(max_retries):
            try:
                logger.info(f"Obteniendo user agents de {url} (intento {attempt + 1})")
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                
                lines = response.text.strip().split('\n')
                valid_user_agents = []
                
                for line in lines:
                    line = line.strip()
                    # Filtrar user agents móviles y problemáticos
                    if (line and 
                        not any(mobile in line for mobile in ['Android', 'iPhone', 'iPad', 'Mobile']) and
                        'Mozilla/' in line):
                        valid_user_agents.append(line)
                
                all_user_agents.extend(valid_user_agents)
                logger.info(f"Obtenidos {len(valid_user_agents)} user agents de {url}")
                break  # Éxito, salir del bucle de reintentos
                
            except requests.RequestException as e:
                logger.warning(f"Error obteniendo user agents de {url} (intento {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # Esperar antes del siguiente intento
            except Exception as e:
                logger.error(f"Error inesperado con {url}: {e}")
                break
    
    # Si no se obtuvieron user agents, usar valores por defecto
    if not all_user_agents:
        logger.warning("No se pudieron obtener user agents, usando valores por defecto")
        from ..config.config import DEFAULT_USER_AGENTS
        all_user_agents = DEFAULT_USER_AGENTS
    
    # Eliminar duplicados
    unique_user_agents = list(dict.fromkeys(all_user_agents))
    logger.info(f"Total de user agents únicos: {len(unique_user_agents)}")
    
    return unique_user_agents
----- [Archivo: main.py] -----
"""
Punto de entrada principal del proxy server
"""
import asyncio
import logging
import threading
import time
from api.server import start_grpc_server
from internal.proxy.proxy import ProxyValidator
from internal.config.config import UPDATE_TIME_MINUTES

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def reload_proxies_background():
    """Función para recargar proxies en segundo plano"""
    proxy_validator = ProxyValidator()
    
    while True:
        try:
            time.sleep(UPDATE_TIME_MINUTES * 60)
            logger.info("Iniciando recarga de proxies...")
            
            new_proxy_map = proxy_validator.get_valid_proxies()
            total_proxies = sum(len(proxies) for proxies in new_proxy_map.values())
            
            logger.info(f"Proxies válidos refrescados: {total_proxies}")
            
        except Exception as e:
            logger.error(f"Error recargando proxies: {e}")

def main():
    """Función principal"""
    logger.info("Iniciando Proxy Server Python con Selenium")
    
    # Iniciar el servidor gRPC en un hilo separado
    grpc_thread = threading.Thread(target=start_grpc_server, daemon=True)
    grpc_thread.start()
    
    # Iniciar la recarga de proxies en segundo plano
    reload_thread = threading.Thread(target=reload_proxies_background, daemon=True)
    reload_thread.start()
    
    try:
        # Mantener la aplicación en ejecución
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("Cerrando servidor...")

if __name__ == "__main__":
    main()
----- [Archivo: protos/proxy.proto] -----
syntax = "proto3";

package proxy;

service ProxyService {
    rpc FetchContent(Request) returns (Response);
    rpc GetRandomProxy(ProxyRequest) returns (ProxyResponse);
    rpc GetProxyStats(StatsRequest) returns (StatsResponse);
}

message Request {
    string url = 1;
    string session = 2;
    bool proxy = 3;
    bool redirect = 4;
}

message Response {
    bytes content = 1;
}

message ProxyRequest {
    string session = 1;
}

message ProxyResponse {
    string proxy = 1;
    bool success = 2;
    string message = 3;
}

message StatsRequest {
    // Vacío por ahora
}

message StatsResponse {
    map<string, int32> proxy_count_by_session = 1;
    int32 total_valid_proxies = 2;
}
----- [Archivo: proxy_pb2_grpc.py] -----
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc
import warnings

import proxy_pb2 as proxy__pb2

GRPC_GENERATED_VERSION = '1.74.0'
GRPC_VERSION = grpc.__version__
_version_not_supported = False

try:
    from grpc._utilities import first_version_is_lower
    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
except ImportError:
    _version_not_supported = True

if _version_not_supported:
    raise RuntimeError(
        f'The grpc package installed is at version {GRPC_VERSION},'
        + f' but the generated code in proxy_pb2_grpc.py depends on'
        + f' grpcio>={GRPC_GENERATED_VERSION}.'
        + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
        + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
    )


class ProxyServiceStub(object):
    """Missing associated documentation comment in .proto file."""

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.FetchContent = channel.unary_unary(
                '/proxy.ProxyService/FetchContent',
                request_serializer=proxy__pb2.Request.SerializeToString,
                response_deserializer=proxy__pb2.Response.FromString,
                _registered_method=True)
        self.GetRandomProxy = channel.unary_unary(
                '/proxy.ProxyService/GetRandomProxy',
                request_serializer=proxy__pb2.ProxyRequest.SerializeToString,
                response_deserializer=proxy__pb2.ProxyResponse.FromString,
                _registered_method=True)
        self.GetProxyStats = channel.unary_unary(
                '/proxy.ProxyService/GetProxyStats',
                request_serializer=proxy__pb2.StatsRequest.SerializeToString,
                response_deserializer=proxy__pb2.StatsResponse.FromString,
                _registered_method=True)


class ProxyServiceServicer(object):
    """Missing associated documentation comment in .proto file."""

    def FetchContent(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GetRandomProxy(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GetProxyStats(self, request, context):
        """Missing associated documentation comment in .proto file."""
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_ProxyServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'FetchContent': grpc.unary_unary_rpc_method_handler(
                    servicer.FetchContent,
                    request_deserializer=proxy__pb2.Request.FromString,
                    response_serializer=proxy__pb2.Response.SerializeToString,
            ),
            'GetRandomProxy': grpc.unary_unary_rpc_method_handler(
                    servicer.GetRandomProxy,
                    request_deserializer=proxy__pb2.ProxyRequest.FromString,
                    response_serializer=proxy__pb2.ProxyResponse.SerializeToString,
            ),
            'GetProxyStats': grpc.unary_unary_rpc_method_handler(
                    servicer.GetProxyStats,
                    request_deserializer=proxy__pb2.StatsRequest.FromString,
                    response_serializer=proxy__pb2.StatsResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'proxy.ProxyService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))
    server.add_registered_method_handlers('proxy.ProxyService', rpc_method_handlers)


 # This class is part of an EXPERIMENTAL API.
class ProxyService(object):
    """Missing associated documentation comment in .proto file."""

    @staticmethod
    def FetchContent(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(
            request,
            target,
            '/proxy.ProxyService/FetchContent',
            proxy__pb2.Request.SerializeToString,
            proxy__pb2.Response.FromString,
            options,
            channel_credentials,
            insecure,
            call_credentials,
            compression,
            wait_for_ready,
            timeout,
            metadata,
            _registered_method=True)

    @staticmethod
    def GetRandomProxy(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(
            request,
            target,
            '/proxy.ProxyService/GetRandomProxy',
            proxy__pb2.ProxyRequest.SerializeToString,
            proxy__pb2.ProxyResponse.FromString,
            options,
            channel_credentials,
            insecure,
            call_credentials,
            compression,
            wait_for_ready,
            timeout,
            metadata,
            _registered_method=True)

    @staticmethod
    def GetProxyStats(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(
            request,
            target,
            '/proxy.ProxyService/GetProxyStats',
            proxy__pb2.StatsRequest.SerializeToString,
            proxy__pb2.StatsResponse.FromString,
            options,
            channel_credentials,
            insecure,
            call_credentials,
            compression,
            wait_for_ready,
            timeout,
            metadata,
            _registered_method=True)

----- [Archivo: proxy_pb2.py] -----
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: proxy.proto
# Protobuf Python Version: 6.31.1
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    6,
    31,
    1,
    '',
    'proxy.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0bproxy.proto\x12\x05proxy\"H\n\x07Request\x12\x0b\n\x03url\x18\x01 \x01(\t\x12\x0f\n\x07session\x18\x02 \x01(\t\x12\r\n\x05proxy\x18\x03 \x01(\x08\x12\x10\n\x08redirect\x18\x04 \x01(\x08\"\x1b\n\x08Response\x12\x0f\n\x07\x63ontent\x18\x01 \x01(\x0c\"\x1f\n\x0cProxyRequest\x12\x0f\n\x07session\x18\x01 \x01(\t\"@\n\rProxyResponse\x12\r\n\x05proxy\x18\x01 \x01(\t\x12\x0f\n\x07success\x18\x02 \x01(\x08\x12\x0f\n\x07message\x18\x03 \x01(\t\"\x0e\n\x0cStatsRequest\"\xb7\x01\n\rStatsResponse\x12M\n\x16proxy_count_by_session\x18\x01 \x03(\x0b\x32-.proxy.StatsResponse.ProxyCountBySessionEntry\x12\x1b\n\x13total_valid_proxies\x18\x02 \x01(\x05\x1a:\n\x18ProxyCountBySessionEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x05:\x02\x38\x01\x32\xb8\x01\n\x0cProxyService\x12/\n\x0c\x46\x65tchContent\x12\x0e.proxy.Request\x1a\x0f.proxy.Response\x12;\n\x0eGetRandomProxy\x12\x13.proxy.ProxyRequest\x1a\x14.proxy.ProxyResponse\x12:\n\rGetProxyStats\x12\x13.proxy.StatsRequest\x1a\x14.proxy.StatsResponseb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'proxy_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_STATSRESPONSE_PROXYCOUNTBYSESSIONENTRY']._loaded_options = None
  _globals['_STATSRESPONSE_PROXYCOUNTBYSESSIONENTRY']._serialized_options = b'8\001'
  _globals['_REQUEST']._serialized_start=22
  _globals['_REQUEST']._serialized_end=94
  _globals['_RESPONSE']._serialized_start=96
  _globals['_RESPONSE']._serialized_end=123
  _globals['_PROXYREQUEST']._serialized_start=125
  _globals['_PROXYREQUEST']._serialized_end=156
  _globals['_PROXYRESPONSE']._serialized_start=158
  _globals['_PROXYRESPONSE']._serialized_end=222
  _globals['_STATSREQUEST']._serialized_start=224
  _globals['_STATSREQUEST']._serialized_end=238
  _globals['_STATSRESPONSE']._serialized_start=241
  _globals['_STATSRESPONSE']._serialized_end=424
  _globals['_STATSRESPONSE_PROXYCOUNTBYSESSIONENTRY']._serialized_start=366
  _globals['_STATSRESPONSE_PROXYCOUNTBYSESSIONENTRY']._serialized_end=424
  _globals['_PROXYSERVICE']._serialized_start=427
  _globals['_PROXYSERVICE']._serialized_end=611
# @@protoc_insertion_point(module_scope)

----- [Archivo: README.md] -----
## README.md
# Proxy Server Python con Selenium

Un servidor proxy HTTP/HTTPS implementado en Python que utiliza Selenium para validación de proxies y obtención de contenido web. Este proyecto es una implementación equivalente del proxy server original en Go, pero aprovechando las capacidades de Selenium para mayor compatibilidad con sitios web modernos.

## Características

- **Servidor gRPC** con múltiples métodos de servicio
- **Validación de proxies con Selenium** para mayor precisión
- **Sistema de sesiones** configurables por sitio web
- **Scraping automático** de proxies y user agents
- **Fallback inteligente** entre Selenium y requests
- **Contenedorización con Docker**
- **Actualización automática** de proxies en segundo plano

## Estructura del Proyecto

```
proxy-server-python/
├── requirements.txt          # Dependencias Python
├── docker-compose.yml        # Configuración Docker Compose
├── Dockerfile               # Imagen Docker
├── main.py                  # Punto de entrada principal
├── protos/
│   └── proxy.proto          # Definición del servicio gRPC
├── api/
│   └── server.py            # Implementación del servidor gRPC
├── internal/
│   ├── config/
│   │   ├── config.py        # Configuración general
│   │   └── sessions.py      # Configuración de sesiones
│   ├── proxy/
│   │   └── proxy.py         # Validación de proxies con Selenium
│   └── scraper/
│       └── scraper.py       # Scraping de proxies y user agents
└── scripts/
    ├── generate_proto.sh    # Script para generar archivos proto
    └── test_client.py       # Cliente de prueba
```

## Instalación y Uso

### Método 1: Docker Compose (Recomendado)

```bash
# Clonar y entrar al directorio
git clone <repository>
cd proxy-server-python

# Ejecutar con Docker Compose
docker-compose up --build
```

### Método 2: Instalación Local

```bash
# Instalar dependencias
pip install -r requirements.txt

# Generar archivos gRPC
chmod +x scripts/generate_proto.sh
./scripts/generate_proto.sh

# Ejecutar servidor
python main.py
```

### Método 3: Docker Manual

```bash
# Construir imagen
docker build -t proxy-server-python .

# Ejecutar contenedor
docker run -d -p 5000:5000 --name proxy_server proxy-server-python
```

## Uso del Cliente

```python
import grpc
import proxy_pb2
import proxy_pb2_grpc

# Conectar al servidor
channel = grpc.insecure_channel('localhost:5000')
stub = proxy_pb2_grpc.ProxyServiceStub(channel)

# Obtener contenido con proxy
request = proxy_pb2.Request(
    url="https://httpbin.org/ip",
    session="TestSession",
    proxy=True,
    redirect=True
)
response = stub.FetchContent(request)
print(response.content.decode())

# Obtener proxy aleatorio
proxy_request = proxy_pb2.ProxyRequest(session="TestSession")
proxy_response = stub.GetRandomProxy(proxy_request)
print(f"Proxy: {proxy_response.proxy}")

# Obtener estadísticas
stats_request = proxy_pb2.StatsRequest()
stats_response = stub.GetProxyStats(stats_request)
print(f"Total proxies: {stats_response.total_valid_proxies}")
```

## Configuración de Sesiones

Las sesiones permiten configurar diferentes parámetros para distintos sitios web:

```python
PROXY_SESSIONS = {
    "MiSitio": ProxySession(
        name="MiSitio",
        url="https://ejemplo.com/api",
        headers={
            "Authorization": "Bearer token",
            "Accept": "application/json",
        },
        timeout=5000,  # milisegundos
    )
}
```

## API gRPC

### Métodos Disponibles

1. **FetchContent**: Obtiene contenido de una URL usando proxies
2. **GetRandomProxy**: Obtiene un proxy aleatorio válido para una sesión
3. **GetProxyStats**: Obtiene estadísticas de proxies por sesión

### Mensajes

- `Request`: URL, sesión, usar proxy, permitir redirects
- `Response`: Contenido en bytes
- `ProxyRequest`: Sesión
- `ProxyResponse`: Proxy, éxito, mensaje
- `StatsRequest`: Vacío
- `StatsResponse`: Estadísticas por sesión

## Diferencias con la Versión Go

### Ventajas de la Versión Python

- **Selenium WebDriver**: Mejor manejo de JavaScript y sitios modernos
- **Flexibilidad**: Más fácil de modificar y extender
- **Ecosistema Python**: Acceso a librerías especializadas
- **Debugging**: Herramientas de debugging más avanzadas

### Consideraciones de Rendimiento

- **Memoria**: Selenium consume más memoria que requests simples
- **Velocidad**: Selenium es más lento pero más preciso
- **Fallback**: Sistema de fallback a requests para mejor rendimiento

## Monitoreo y Logs

El servidor proporciona logs detallados:

```
2024-01-15 10:30:00 - INFO - Iniciando Proxy Server Python con Selenium
2024-01-15 10:30:01 - INFO - Servidor gRPC iniciado en [::]:5000
2024-01-15 10:30:05 - INFO - Validando 150 proxies...
2024-01-15 10:32:10 - INFO - Validación completada. Total de proxies válidos: 25
```

## Troubleshooting

### Chrome/ChromeDriver Issues

```bash
# Instalar Chrome manualmente
wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -
echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list
apt-get update && apt-get install -y google-chrome-stable
```

### Memoria Insuficiente

```bash
# Aumentar memoria compartida en Docker
docker run --shm-size=2g -p 5000:5000 proxy-server-python
```

### Permisos

```bash
# Dar permisos a scripts
chmod +x scripts/*.sh
```

## Contribuir

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## Licencia

MIT License - ver archivo LICENSE para detalles.
----- [Archivo: scripts/generate_proto.sh] -----
#!/usr/bin/env bash
set -euo pipefail

# Nos movemos al directorio raíz (uno arriba de scripts/)
cd "$(dirname "${BASH_SOURCE[0]}")/.."

PROTO_DIR="protos"
OUT_DIR="."

# Verificar que exista el .proto
if [ ! -f "${PROTO_DIR}/proxy.proto" ]; then
  echo "Error: No se encontró ${PROTO_DIR}/proxy.proto"
  exit 1
fi

# Generar los bindings de Python
python -m grpc_tools.protoc \
  --proto_path="${PROTO_DIR}" \
  --python_out="${OUT_DIR}" \
  --grpc_python_out="${OUT_DIR}" \
  "${PROTO_DIR}/proxy.proto"

echo "✅ Archivos Python generados en ${OUT_DIR}:"
echo "   - proxy_pb2.py"
echo "   - proxy_pb2_grpc.py"

----- [Archivo: script.sh] -----
#!/usr/bin/env bash

# Nombre del archivo de salida
OUTPUT="esquema.txt"

# Patrones a excluir (incluye el propio OUTPUT)
EXCLUDE_PATTERNS=(
  "$OUTPUT"        # No incluir el archivo de salida
  ".git"           # Control de versiones
  "node_modules"   # Dependencias de Node.js
  ".DS_Store"      # Metadatos en macOS
  "Thumbs.db"      # Metadatos en Windows
  "__pycache__"    # Cachés de Python
  "*.pyc"          # Archivos compilados de Python
  "*.o"            # Objetos compilados (C/C++)
  "*.so"           # Librerías compartidas
  "venv"           # Entorno virtual de Python
  "dist"           # Carpeta de distribución
  "proxyserver"      # Archivos de caché de fetch
  "fetch"         # Archivos de caché de fetch
  ".gitignore"    # Ignorar archivos de configuración de git
  "main"          # Archivos de caché de fetch
  "requirements.txt" # Archivo de dependencias
  "input"         # Archivo de entrada para SCM
)

# 1) Truncar o crear el archivo de salida sin preguntar
: > "$OUTPUT"

# 2) Construir la expresión de prune para find
prune_args=()
for pat in "${EXCLUDE_PATTERNS[@]}"; do
  prune_args+=( -name "$pat" -o )
done
# Eliminar el último '-o'
unset 'prune_args[${#prune_args[@]}-1]'

# 3) Estructura de directorios y archivos
echo "=== Estructura de directorios y archivos ===" >> "$OUTPUT"
find . -mindepth 1 \( "${prune_args[@]}" \) -prune -o -print | \
  sed \
    -e 's|[^/]*/|│   |g' \
    -e 's|│   \([^│]\)|├── \1|' \
  >> "$OUTPUT"
echo >> "$OUTPUT"

# 4) Contenido de cada archivo
echo "=== Contenido de archivos ===" >> "$OUTPUT"
find . -mindepth 1 \( "${prune_args[@]}" \) -prune -o \( -type f ! -name "$OUTPUT" -print \) | sort | \
while IFS= read -r file; do
  rel="${file#./}"
  echo "----- [Archivo: $rel] -----" >> "$OUTPUT"
  if [ -r "$file" ] && [ ! -d "$file" ]; then
    cat "$file" >> "$OUTPUT"
  else
    echo "[No se puede leer: $rel]" >> "$OUTPUT"
  fi
  echo >> "$OUTPUT"
done

